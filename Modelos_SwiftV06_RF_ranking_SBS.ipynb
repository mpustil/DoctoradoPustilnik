{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mpustil/DoctoradoPustilnik/blob/main/Modelos_SwiftV06_RF_ranking_SBS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk0eXDxqlXzd"
      },
      "outputs": [],
      "source": [
        "# Modelo 06: ranking SBS  (1/12/2025)\n",
        "# preprocesamieto - imputacion lineal\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "'''\n",
        "Archivos utilizados\n",
        "Swift05_11SBS165.csv\n",
        "https://drive.google.com/file/d/1T53fIqyrr70CHqXvB5lp0deJTxrfHUfV/view?usp=sharing\n",
        "Swift05_11SBS52.csv\n",
        "https://drive.google.com/file/d/1dQ5puACEkvwEE3zOl1vIrPglVQHXO7gD/view?usp=sharing\n",
        "\n",
        "Swift11NoSBS.csv (solo registros sin flags de error)\n",
        "https://drive.google.com/file/d/1-kSWbAy4ZL73eb6Wik3cei1s4iA1H1-j/view?usp=drive_link\n",
        "\n",
        "'''\n",
        "# To obtain a deterministic behaviour during subsampling and fitting, random_state has to be fixed to an integer\n",
        "random_state = 41\n",
        "\n",
        "ruta = \"/content/drive/MyDrive/Tesis Doctorado Pustilnik/Swift/\"\n",
        "\n",
        "#preprocesamiento y union de los dataset. Variable target: SBS sii df['52']==1\n",
        "def prepM3(archivo, clase, archivo2 = ''):\n",
        "  df = pd.read_csv(archivo, delimiter = \";\")\n",
        "\n",
        "  if archivo2 != '':\n",
        "    df2 = pd.read_csv(archivo2, delimiter = \";\")\n",
        "    df = pd.concat([df,df2])\n",
        "    del df2\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "  cols = ['2', '4']\n",
        "  df[cols] = df[cols].applymap(np.int64)\n",
        "  #variable target\n",
        "  df['52'] = clase\n",
        "\n",
        "  df.drop(['2', '112', '113', '114', '115'], axis=1, inplace=True)\n",
        "  if clase == 1:\n",
        "    #df.drop(['50','51','53'], axis=1, inplace=True)\n",
        "    #Nos quedamos con el nombre\n",
        "    df.drop(['50','51'], axis=1, inplace=True)\n",
        "  else:\n",
        "    df['53'] = ''\n",
        "\n",
        "  # ponemos los NA\n",
        "  listaVariables = ['110','111']\n",
        "  df[listaVariables] = df[listaVariables].replace(0, np.nan)\n",
        "\n",
        "  #que tenga al menos uno de los filtros\n",
        "  df = df[(df['110'] >= 0) | (df['111'] >=0 ) ]\n",
        "  if clase == 0:\n",
        "  #que no tenga flags\n",
        "     df = df[(df['20'] == 0) & (df['21'] ==0 ) ]\n",
        "     df.drop(['20','21'], axis=1, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------imputacion lineal (en datos sin resumir)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import math\n",
        "\n",
        "#y = ax+b  1 variable por ahora\n",
        "def imputarLineal(x ,y, a, b):\n",
        "    if math.isnan(y):\n",
        "      ret = a*x+b\n",
        "    else:\n",
        "      ret = y\n",
        "    return ret\n",
        "\n",
        "#Imputacion por regresion lineal\n",
        "def regresionLineal(df):\n",
        "  # Dropping the entire rows with missing values\n",
        "  df_drop = df.dropna()\n",
        "  #print(\"total con 110 y 111 \",len(df_drop))\n",
        "  #print(df_drop)\n",
        "\n",
        "  # Fitting the linear regression model\n",
        "  X = df_drop[['110']]\n",
        "  y = df_drop['111']\n",
        "  model110 = LinearRegression().fit(X, y)\n",
        "\n",
        "  X = df_drop[['111']]\n",
        "  y = df_drop['110']\n",
        "  model111 = LinearRegression().fit(X, y)\n",
        "\n",
        "  #print(f\"Coefficients: {model110.coef_[0]}, Intercept: {model110.intercept_}\")\n",
        "  #print(f\"Coefficients: {model111.coef_[0]}, Intercept: {model111.intercept_}\")\n",
        "\n",
        "  #print()\n",
        "  #print(df.corr())\n",
        "  df['111'] = df.apply(lambda x: imputarLineal(x['110'], x['111'], model110.coef_[0], model110.intercept_), axis=1)\n",
        "  df['110'] = df.apply(lambda x: imputarLineal(x['111'], x['110'], model111.coef_[0], model111.intercept_), axis=1)\n",
        "\n",
        "\n",
        "\n",
        "#-------------------------- Pre-procesamiento parte 2: submuestreo y sobremuestreo (codigo). ya viene todo imputado!\n",
        "\n",
        "# importamos librerias para balancear los datos\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "#SMOTE (Synthetic Minority Over-sampling Technique)\n",
        "#https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "def subMuestrear(df, _random_state):\n",
        "\n",
        "  # vamos a dejar un 10% de SBS (sampling_strategy = 0.1) y 90% NoSBS, para luego, hacer oversamplig de SBS...\n",
        "  #undersample = RandomUnderSampler(random_state=_random_state, sampling_strategy = 0.1)\n",
        "\n",
        "  # vamos a dejar igual peso, 50% de SBS, pporque vamos a repetir esto muchas veces\n",
        "  undersample = RandomUnderSampler(random_state=_random_state, sampling_strategy = 1)\n",
        "  #oversample = SMOTE(random_state=random_state)\n",
        "\n",
        "  # Separamos en X e y\n",
        "  X_SBS = df.drop(['52'], axis=1)\n",
        "  y_SBS = df['52']\n",
        "\n",
        "  #hacemos el submuestreo\n",
        "  X_over_SBS, y_over_SBS = undersample.fit_resample(X_SBS , y_SBS)\n",
        "\n",
        "\n",
        "  # preparamos todo para agrupar despues\n",
        "  df = X_over_SBS\n",
        "  df['52'] = y_over_SBS\n",
        "  dfSBS2 = df[(df['52'] == 1)]\n",
        "  dfNoSBS2 = df[(df['52'] == 0)]\n",
        "  dfSBS2.drop(['52'], axis=1,inplace= True)\n",
        "  dfNoSBS2.drop(['52'], axis=1,inplace= True)\n",
        "\n",
        "  return dfSBS2, dfNoSBS2\n",
        "\n",
        "#print(len(dfSBS2), len(dfNoSBS2), len(X_over_SBS), len(df))\n",
        "\n",
        "# Agrupador prepM3b  (y submuestreo del modelo agrupado)\n",
        "\n",
        "def desvio0(min, max, desvio)->float:\n",
        "  ret = float('nan')\n",
        "  if not math.isnan(min):\n",
        "    ret = desvio\n",
        "    if min == max:\n",
        "      ret = 0.0\n",
        "  return ret\n",
        "\n",
        "\n",
        "#preprocesamiento y union de los dataset. Variable target: SBS sii df['52']==1\n",
        "def prepM3b(df, clase):\n",
        "\n",
        "  #2: obsid, 4: fuente   mantenemos la fuente, para poder hacer el ranking\n",
        "#   df = df.groupby(['4'],group_keys=True).agg(\n",
        "  df = df.groupby(['4'],group_keys=True).agg(\n",
        "      #count=('2','count'),  # cant obs\n",
        "\n",
        "      SBS=('53', 'min'),\n",
        "      min110=('110', 'min'),\n",
        "      max110=('110', 'max'),\n",
        "      mean110=('110', 'mean'),\n",
        "      std110=('110', 'std'),\n",
        "\n",
        "      min111=('111', 'min'),\n",
        "      max111=('111', 'max'),\n",
        "      mean111=('111', 'mean'),\n",
        "      std111=('111', 'std'))\n",
        "\n",
        "  #corregimos el desvio muestral (cuando hay 1 sola muestra)\n",
        "  df['std110'] = df.apply(lambda x: desvio0(x['min110'],x['max110'],x['std110']), axis=1)\n",
        "  df['std111'] = df.apply(lambda x: desvio0(x['min111'],x['max111'],x['std111']), axis=1)\n",
        "\n",
        "  #variable target\n",
        "  df['52'] = clase\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def prep():\n",
        "  dfSBS = prepM3(ruta + 'Swift05_11SBS165.csv', 1, ruta + 'Swift05_11SBS52.csv')\n",
        "  dfNoSBS = prepM3(ruta + 'Swift11NoSBS.csv', 0)\n",
        "\n",
        "  #imputacion por regresion lineal (en toda la base)\n",
        "  regresionLineal(dfSBS)\n",
        "  regresionLineal(dfNoSBS)\n",
        "\n",
        "  frames = [dfSBS, dfNoSBS]\n",
        "  return pd.concat(frames)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#------------------------------------------------ split y entrenamiento\n",
        "#  _random_state para sub muestrear diferente que el entrenamiento (random_state, global, es para entrenar)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import tree\n",
        "\n",
        "sns.set(context='notebook')\n",
        "\n",
        "#  _random_state para muestrear diferente que el entrenamiento (random_state <-- global)\n",
        "def RF(df, _random_state, _class_weight='balanced'):\n",
        "\n",
        "  dfSBS2, dfNoSBS2 = subMuestrear(df, _random_state)\n",
        "\n",
        "  #agrupamiento\n",
        "  dfSBS2 = prepM3b(dfSBS2,1)\n",
        "  dfNoSBS2 = prepM3b(dfNoSBS2,0)\n",
        "\n",
        "  #print('dfSBS2',len(dfSBS2), 'dfNoSBS2', len(dfNoSBS2))\n",
        "\n",
        "  frames = [dfSBS2, dfNoSBS2]\n",
        "  df2 = pd.concat(frames)\n",
        "  # Separamos en X e y. Quitamos 'SBS'\n",
        "  X_SBS = df2.drop(['52','SBS'], axis=1)\n",
        "  y_SBS = df2['52']\n",
        "\n",
        "  # 30% para test y 70% para train\n",
        "  X_train_SBS, X_test_SBS, y_train_SBS, y_test_SBS = train_test_split(X_SBS,y_SBS, test_size=0.30,random_state=_random_state)\n",
        "\n",
        "  print('X_train_SBS',len(X_train_SBS), 'X_test_SBS', len(X_test_SBS))\n",
        "\n",
        "  #notar que random_state es global y fijo para todo el experimiento\n",
        "  clf = RandomForestClassifier(random_state=random_state, class_weight= _class_weight)\n",
        "\n",
        "  param_grid = {'n_estimators':[10,20,30], 'criterion': ['gini', 'entropy'], 'max_depth': [3,4,5,6,7,8], 'min_samples_leaf' : [3]} #, 'min_samples_leaf' : [5]}\n",
        "\n",
        "  # Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
        "  grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, return_train_score=True)\n",
        "\n",
        "  grid_search.fit(X_train_SBS, y_train_SBS)\n",
        "\n",
        "  return grid_search, X_train_SBS, X_test_SBS, y_test_SBS, dfSBS2\n",
        "\n",
        "\n",
        "#Metricas y Graficamos matriz de confusion\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def imprimirMetricas(mcTexto, mcGrafico, redondeo, y_train_SBS, X_test_SBS, y_test_SBS,y_test_pred_SBS, best_clf, soloF1, pesoSBS = None):\n",
        "\n",
        "  cm = confusion_matrix(y_test_SBS,y_test_pred_SBS,labels=best_clf.classes_)\n",
        "  TN, FP, FN, TP = cm.ravel()\n",
        "  #  (con las clases 1,0)\n",
        "  if mcGrafico:\n",
        "    ConfusionMatrixDisplay(cm, display_labels=np.flip(best_clf.classes_)).plot()\n",
        "\n",
        "  # Calculo de las predicciones en Train y test\n",
        "  y_train_pred = best_clf.predict(X_train_SBS)\n",
        "  y_test_pred = best_clf.predict(X_test_SBS)\n",
        "\n",
        "  if not soloF1:\n",
        "    #print('El accuracy en train es:',round(accuracy_score(y_train_SBS,y_train_pred_SBS),redondeo))\n",
        "    print('El accuracy en test es:', round(accuracy_score(y_test_SBS,y_test_pred_SBS),redondeo))\n",
        "\n",
        "  #print(classification_report(y_test_SBS,y_test_pred_SBS))\n",
        "  accu = (TP + TN) / (TP + FP + FN + TN)\n",
        "  pres = TP / (TP + FP)\n",
        "  rec = TP / (TP + FN)  # TP / (FN + TP)\n",
        "  f1 = 2 * pres * rec / (pres + rec)\n",
        "\n",
        "  if soloF1:\n",
        "    if pesoSBS != None:\n",
        "      print(\"pesoSBS\", round(pesoSBS, redondeo), \"F1\", round(f1,redondeo))\n",
        "    else:\n",
        "      print( \"F1\", round(f1,redondeo))\n",
        "  else:\n",
        "    if mcTexto:\n",
        "      print(\"Matriz de confusión\")\n",
        "      print(\"Predicho (1) (0)\")\n",
        "      print(\"Real(1) \",TP,\"  \", FN)\n",
        "      print(\"Real(0) \",FP,\"  \", TN)\n",
        "\n",
        "    print(\"Accuracy :  \", round(accu,redondeo))\n",
        "    print(\"Precision : \", round(pres,redondeo))\n",
        "    print(\"Recall :    \", round(rec,redondeo))\n",
        "    print(\"F1 :        \", round(f1,redondeo))\n",
        "\n",
        "\n",
        "def agregarSBSNombres(X_test_SBS, dfSBS2, y_test_pred_SBS):\n",
        "#le agregamos los nombres de las SBS y la prediccion\n",
        "  X_test_SBS = X_test_SBS.merge(dfSBS2, how='left', on='4')\n",
        "\n",
        "  X_test_SBS.drop(['min110_y','min111_y','max110_y','max111_y','mean110_y','mean111_y','std110_y','std111_y'], axis=1, inplace=True)\n",
        "\n",
        "  X_test_SBS['pred'] = y_test_pred_SBS\n",
        "\n",
        "  soloSBS = X_test_SBS[X_test_SBS['SBS'].notna()]\n",
        "\n",
        "  return soloSBS\n"
      ],
      "metadata": {
        "id": "3m2n4Vbsaqul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#------------empezar aca\n",
        "\n",
        "\n",
        "df = prep()\n",
        "pesoSBS = 0.91 #peso optimo surge de experimento anterior para el caso desbalceado solamente\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp7pluJUclFM",
        "outputId": "20925e4d-8294-4443-de24-a883be717d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2825026973.py:36: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df[cols] = df[cols].applymap(np.int64)\n",
            "/tmp/ipython-input-2825026973.py:36: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df[cols] = df[cols].applymap(np.int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# split y entrenamiento. en cada ciclo\n",
        "# por ahora \"igual peso\". sino poner:  {1:pesoSBS, 0: 1-pesoSBS}\n",
        "\n",
        "\n",
        "cantCorridas = 20 #se toman semillas (random_state) de 0 a cantCorridas\n",
        "resultados = np.empty(cantCorridas, dtype = pd.DataFrame)\n",
        "\n",
        "i=0\n",
        "for random_state_i in range(cantCorridas):\n",
        "  grid_search, X_train_SBS, X_test_SBS, y_test_SBS, dfSBS2 = RF(df, random_state_i)\n",
        "\n",
        "    # Modelo decision tree con parametros optimizados\n",
        "  best_clf = grid_search.best_estimator_\n",
        "\n",
        "    # Predecimos Y\n",
        "  y_train_pred_SBS = best_clf.predict(X_train_SBS)\n",
        "  y_test_pred_SBS = best_clf.predict(X_test_SBS)\n",
        "\n",
        "  #xxx opcional\n",
        "  #imprimirMetricas(True, False, 4,X_train_SBS, X_test_SBS, y_test_SBS, y_test_pred_SBS, best_clf, False)\n",
        "\n",
        "  #me guardo la prediccion de todas las SBS en cada iteracion\n",
        "  resultados[i]= agregarSBSNombres(X_test_SBS, dfSBS2, y_test_pred_SBS)\n",
        "  print(\"dataframe \", i)\n",
        "  i = i +1\n",
        "\n"
      ],
      "metadata": {
        "id": "UUBiVizsXg7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#estadisticas\n",
        "\n",
        "dic = {}\n",
        "\n",
        "#recorro todos los dataframes con las clasificaciones de SBS\n",
        "for i in range(len(resultados)):\n",
        "\n",
        "  for index, row in resultados[i].iterrows():\n",
        "    if int(row['52'])== int(row['pred']):\n",
        "      if row['SBS'] in dic:\n",
        "        dic[row['SBS']] = (dic[row['SBS']][0]+1,dic[row['SBS']][1])\n",
        "      else:\n",
        "        dic[row['SBS']] = (1,0)\n",
        "    else:\n",
        "      if row['SBS'] in dic:\n",
        "        dic[row['SBS']] = (dic[row['SBS']][0],dic[row['SBS']][1]+1)\n",
        "      else:\n",
        "        dic[row['SBS']] = (0,1)\n",
        "\n",
        "#for clave in dic.keys():\n",
        "#  print(clave, dic[clave])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrBOCFAurKXj",
        "outputId": "47eec690-fd46-4b69-a5c4-9eebc01a589d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EM* AS  453 (6, 0)\n",
            "V* AX Per (4, 0)\n",
            "M33SyS J013239.11+303836.5 (0, 6)\n",
            "V* RW Hya (3, 0)\n",
            "EM* AS  297 (5, 0)\n",
            "V* ZZ CMi (9, 0)\n",
            "V* ER Del (6, 0)\n",
            "V* V420 Hya (8, 0)\n",
            "V* CN Cha (3, 2)\n",
            "M33SyS J013334.95+305108.3 (0, 6)\n",
            "OGLE BLG-LPV-26902 (0, 9)\n",
            "V* SY Mus (7, 0)\n",
            "M31SyS J004335.01+414358.2 (0, 7)\n",
            "NAME UKS-Ce1 (2, 2)\n",
            "V* RS Oph (7, 0)\n",
            "EM* MWC  603 (8, 0)\n",
            "V* AR Pav (6, 0)\n",
            "V* KX TrA (7, 0)\n",
            "V* RT Cru (5, 0)\n",
            "EM* AS  289 (4, 0)\n",
            "CD-28  3719 (9, 0)\n",
            "Haro 1-2 (6, 0)\n",
            "M31SyS J004117.11+404524.1 (0, 7)\n",
            "GSC 06806-00016 (5, 1)\n",
            "V* Z And (9, 0)\n",
            "V* CH Cyg (8, 0)\n",
            "V* TX CVn (8, 0)\n",
            "WRAY 15-1511 (9, 1)\n",
            "BD-21  3873 (7, 0)\n",
            "V* UV Aur (7, 0)\n",
            "EM* StHA  169 (6, 0)\n",
            "V* NQ Gem (8, 0)\n",
            "WRAY 16-148 (6, 1)\n",
            "V* T CrB (7, 0)\n",
            "M33SyS J013303.27+303528.3 (1, 10)\n",
            "LGGS J004216.72+404415.8 (0, 5)\n",
            "NAME SMC Symbiotic star 2 (10, 1)\n",
            "WRAY 15-619 (2, 4)\n",
            "EM* StHA   32 (9, 0)\n",
            "EM* StHA  190 (5, 0)\n",
            "V* LT Del (1, 0)\n",
            "M31SyS J004155.61+410846.7 (5, 1)\n",
            "WRAY 16-208 (7, 2)\n",
            "V* CI Cyg (3, 0)\n",
            "Hen 3-863 (6, 0)\n",
            "V* BI Cru (3, 1)\n",
            "V* V934 Her (5, 1)\n",
            "V* Y CrA (4, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def dict_of_lists_to_csv(dictionary, filename):\n",
        "    keys = list(dictionary.keys())\n",
        "    values = list(dictionary.values())\n",
        "\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(keys) # Write the column headers\n",
        "        writer.writerows(zip(*values)) # Transpose and write the values\n",
        "\n",
        "dict_of_lists_to_csv(dic, 'rankingSBS.csv')"
      ],
      "metadata": {
        "id": "bUrjT1U2gNjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#le agregamos los nombres de las SBS y la prediccion\n",
        "X_test_SBS = X_test_SBS.merge(dfSBS2, how='left', on='4')\n",
        "\n",
        "X_test_SBS.drop(['min110_y','min111_y','max110_y','max111_y','mean110_y','mean111_y','std110_y','std111_y'], axis=1, inplace=True)\n",
        "\n",
        "X_test_SBS['pred'] = y_test_pred_SBS\n",
        "\n",
        "soloSBS = X_test_SBS[X_test_SBS['SBS'].notna()]\n"
      ],
      "metadata": {
        "id": "8tqpeFxpnd2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soloSBS.to_csv(\"SBSPredic.csv\", sep=';', encoding='utf-8',index=False)"
      ],
      "metadata": {
        "id": "UAg0VFo7gByS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ojo, que guarda sin fuentes!\n",
        "dfSBS.to_csv(\"Swift05_11SBS_Agrupado.csv\", sep=';', encoding='utf-8',index=False)\n",
        "dfNoSBS.to_csv(\"Swift05_11NoSBS_Agrupado.csv\", sep=';', encoding='utf-8',index=False)"
      ],
      "metadata": {
        "id": "thihmHeFSTJR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}